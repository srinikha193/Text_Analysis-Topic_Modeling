{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqVNdbNp5GC4IXXcATa5Jy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinikha193/Text_Analysis-Topic_Modeling/blob/main/4modelcomparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install top2vec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiDObL3pb9EG",
        "outputId": "b0147048-cac4-495a-a8b9-c359bf08e045"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec in /usr/local/lib/python3.10/dist-packages (1.0.34)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from top2vec) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from top2vec) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from top2vec) (1.5.2)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from top2vec) (4.3.3)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from top2vec) (0.5.7)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.10/dist-packages (from top2vec) (0.8.39)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (from top2vec) (1.9.3)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->top2vec) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->top2vec) (7.0.5)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.27->top2vec) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.2.0->top2vec) (3.5.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.1->top2vec) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->top2vec) (2024.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud->top2vec) (10.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud->top2vec) (3.8.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.1->top2vec) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->top2vec) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim>=4.0.0->top2vec) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud->top2vec) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX9uQXZ9mBHO",
        "outputId": "8d239317-315e-41b0-a576-349e5b5acddc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bertopic\n",
            "  Downloading bertopic-0.16.4-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.8.39)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (5.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (1.5.2)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from bertopic) (4.66.6)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from bertopic) (0.5.7)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan>=0.8.29->bertopic) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->bertopic) (2024.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=4.7.0->bertopic) (24.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (2.5.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.4.1->bertopic) (10.4.0)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2024.8.30)\n",
            "Downloading bertopic-0.16.4-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bertopic\n",
            "Successfully installed bertopic-0.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSthjImgaZKA",
        "outputId": "dac0c414-3999-42aa-91cb-7a9587fb8c89"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from top2vec import Top2Vec\n",
        "from bertopic import BERTopic\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel, nmf\n",
        "import numpy as np\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the single CSV file\n",
        "#data = pd.read_csv('/content/combined_dataset.csv')\n",
        "data = pd.read_csv('/content/combined_dataset.csv', usecols=['text'])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.strip()  # Remove leading/trailing spaces\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "data['processed_tweets'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Prepare corpus and dictionary for Gensim coherence calculations\n",
        "dictionary = Dictionary(data['processed_tweets'])\n",
        "corpus = [dictionary.doc2bow(text) for text in data['processed_tweets']]\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = vectorizer_tfidf.fit_transform([\" \".join(tweet) for tweet in data['processed_tweets']])\n",
        "\n",
        "# Vectorization using Count Vectorizer\n",
        "vectorizer_count = CountVectorizer(max_features=5000)\n",
        "X_count = vectorizer_count.fit_transform([\" \".join(tweet) for tweet in data['processed_tweets']])\n",
        "\n",
        "# LDA for topic modeling (using Gensim for coherence)\n",
        "gensim_lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=3, random_state=42)\n",
        "lda_model = LatentDirichletAllocation(n_components=3, random_state=42)\n",
        "lda_model.fit(X_count)\n",
        "\n",
        "# NMF for topic modeling (using Gensim for coherence)\n",
        "gensim_nmf_model = nmf.Nmf(corpus=corpus, num_topics=3, id2word=dictionary, random_state=42)\n",
        "nmf_model = NMF(n_components=3, random_state=42)\n",
        "nmf_model.fit(X_tfidf)\n",
        "\n",
        "# Top2Vec model\n",
        "top2vec_model = Top2Vec(documents=[\" \".join(tweet) for tweet in data['processed_tweets']], speed=\"learn\", workers=4)\n",
        "\n",
        "# BERTopic model\n",
        "bertopic_model = BERTopic()\n",
        "topics, probs = bertopic_model.fit_transform([\" \".join(tweet) for tweet in data['processed_tweets']])\n",
        "\n",
        "# Coherence score calculation\n",
        "def calculate_coherence(model, texts, dictionary, corpus, measure='c_v'):\n",
        "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, corpus=corpus, coherence=measure)\n",
        "    return coherence_model.get_coherence()\n",
        "\n",
        "# Fine-tuning and coherence score evaluation\n",
        "best_coherence = 0\n",
        "best_lda_model = None\n",
        "for num_topics in [2, 3, 5, 10]:\n",
        "    for alpha in ['symmetric', 'asymmetric', 0.1, 0.5]:\n",
        "        lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, alpha=alpha, random_state=42)\n",
        "        coherence = calculate_coherence(lda_model, data['processed_tweets'], dictionary, corpus)\n",
        "        print(f\"LDA - Num Topics: {num_topics}, Alpha: {alpha}, Coherence: {coherence}\")\n",
        "\n",
        "        if coherence > best_coherence:\n",
        "            best_coherence = coherence\n",
        "            best_lda_model = lda_model\n",
        "\n",
        "print(f\"Best LDA Coherence: {best_coherence}\")\n",
        "\n",
        "best_nmf_coherence = 0\n",
        "best_nmf_model = None\n",
        "for n_components in [2, 3, 5, 10]:\n",
        "    nmf_model = NMF(n_components=n_components, init='random', random_state=42)\n",
        "    nmf_model.fit(X_tfidf)\n",
        "    topic_words = [[vectorizer_tfidf.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in nmf_model.components_]\n",
        "    gensim_nmf = [dictionary.doc2bow(text) for text in topic_words]\n",
        "    coherence = CoherenceModel(topics=topic_words, texts=data['processed_tweets'], dictionary=dictionary, coherence='c_v').get_coherence()\n",
        "    print(f\"NMF - Components: {n_components}, Coherence: {coherence}\")\n",
        "\n",
        "    if coherence > best_nmf_coherence:\n",
        "        best_nmf_coherence = coherence\n",
        "        best_nmf_model = nmf_model\n",
        "\n",
        "print(f\"Best NMF Coherence: {best_nmf_coherence}\")\n",
        "\n",
        "# Coherence score for Top2Vec\n",
        "top2vec_topics = top2vec_model.get_topics()\n",
        "top2vec_coherence = CoherenceModel(topics=[words for words, _ in top2vec_topics], texts=data['processed_tweets'], dictionary=dictionary, coherence='c_v').get_coherence()\n",
        "print(f\"Top2Vec Coherence Score: {top2vec_coherence}\")\n",
        "\n",
        "# Coherence score for BERTopic\n",
        "bertopic_topics = bertopic_model.get_topic_info()\n",
        "bertopic_topic_words = [bertopic_model.get_topic(i) for i in range(len(bertopic_topics)) if i != -1]\n",
        "bertopic_topic_words = [[word[0] for word in topic[:10]] for topic in bertopic_topic_words]\n",
        "bertopic_coherence = CoherenceModel(topics=bertopic_topic_words, texts=data['processed_tweets'], dictionary=dictionary, coherence='c_v').get_coherence()\n",
        "print(f\"BERTopic Coherence Score: {bertopic_coherence}\")\n",
        "\n",
        "# Explanation:\n",
        "# This code adds coherence score calculations for Top2Vec and BERTopic. The coherence score is used to evaluate\n",
        "#the quality of the topics generated by each model, helping to determine which model performs best for the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WF5FzlOha9e0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}